{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8308ef1eb6fa4cd9a4e876ac869f3ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "# Choose the model\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Save model and tokenizer to Google Drive\n",
    "# save_path = \"../falcon_model\"\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# model.save_pretrained(save_path)\n",
    "\n",
    "# print(f\"Model saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Define text-generation pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=256  # Limit response length\n",
    ")\n",
    "\n",
    "# Wrap pipeline for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON response format\n",
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# MCQ generation prompt\n",
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, create a quiz with {number} multiple-choice questions\n",
    "for {subject} students at a difficulty level of {tone}. Ensure questions follow the text accurately,\n",
    "are not repeated, and match the RESPONSE_JSON format below.\n",
    "\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example input\n",
    "TEXT = \"\"\"Electromagnetic induction is the process of generating an electromotive force (EMF) or voltage across\n",
    "a conductor when exposed to a changing magnetic field. This principle was discovered by Michael Faraday in 1831\n",
    "and is the basis for electrical generators and transformers.\n",
    "\n",
    "According to Faraday‚Äôs Law, the magnitude of the induced EMF is directly proportional to the rate of change\n",
    "of magnetic flux through the conductor. Mathematically, it is expressed as:\n",
    "\n",
    "ùìî = -dŒ¶B/dt\n",
    "\n",
    "where ùìî is the induced EMF, and Œ¶B is the magnetic flux. The negative sign represents Lenz‚Äôs Law, which states\n",
    "that the induced current will always oppose the change in magnetic flux that caused it.\n",
    "\n",
    "There are two main types of electromagnetic induction: mutual induction and self-induction. Mutual induction\n",
    "occurs when a changing current in one coil induces an EMF in a nearby coil, as seen in transformers. Self-induction\n",
    "occurs when a changing current in a coil induces an EMF in the same coil due to its own changing magnetic field.\n",
    "\n",
    "This principle has widespread applications, including power generation, wireless charging, and credit card readers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashd\\AppData\\Local\\Temp\\ipykernel_4900\\4100818057.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)\n",
      "C:\\Users\\yashd\\AppData\\Local\\Temp\\ipykernel_4900\\4100818057.py:13: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = quiz_chain({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Electromagnetic induction is the process of generating an electromotive force (EMF) or voltage across\n",
      "a conductor when exposed to a changing magnetic field. This principle was discovered by Michael Faraday in 1831\n",
      "and is the basis for electrical generators and transformers.\n",
      "\n",
      "According to Faraday‚Äôs Law, the magnitude of the induced EMF is directly proportional to the rate of change\n",
      "of magnetic flux through the conductor. Mathematically, it is expressed as:\n",
      "\n",
      "ùìî = -dŒ¶B/dt\n",
      "\n",
      "where ùìî is the induced EMF, and Œ¶B is the magnetic flux. The negative sign represents Lenz‚Äôs Law, which states\n",
      "that the induced current will always oppose the change in magnetic flux that caused it.\n",
      "\n",
      "There are two main types of electromagnetic induction: mutual induction and self-induction. Mutual induction\n",
      "occurs when a changing current in one coil induces an EMF in a nearby coil, as seen in transformers. Self-induction\n",
      "occurs when a changing current in a coil induces an EMF in the same coil due to its own changing magnetic field.\n",
      "\n",
      "This principle has widespread applications, including power generation, wireless charging, and credit card readers.\n",
      "You are an expert MCQ maker. Given the above text, create a quiz with 5 multiple-choice questions\n",
      "for Electromagnetic Induction students at a difficulty level of simple. Ensure questions follow the text accurately,\n",
      "are not repeated, and match the RESPONSE_JSON format below.\n",
      "\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")\n",
    "\n",
    "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)\n",
    "\n",
    "NUMBER = 5\n",
    "SUBJECT = \"Electromagnetic Induction\"\n",
    "TONE = \"simple\"\n",
    "\n",
    "# Generate MCQs\n",
    "response = quiz_chain({\n",
    "    \"text\": TEXT,\n",
    "    \"number\": NUMBER,\n",
    "    \"subject\": SUBJECT,\n",
    "    \"tone\": TONE,\n",
    "    \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"Generated MCQs:\\n\", response[\"quiz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
